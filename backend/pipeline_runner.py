"""
Main Pipeline Runner
Integrates AgentCollector, AgentSumm, Google Sheets storage, and PDF generation
"""

import os
import sys
import json
from datetime import datetime
from google_storage import GoogleSheetsDB

# Import your existing agents
try:
    from AgentCollector import CustomArticleCollector
    from AgentSumm import ArticleSummarizer
    print("‚úÖ Collector and Summarizer loaded")
except ImportError as e:
    print(f"‚ùå Could not import agents: {e}")
    sys.exit(1)

# Import PDF generator
PDF_AVAILABLE = False
try:
    from PDFGenerator import BiweeklyRoundupPDF
    PDF_AVAILABLE = True
    print("‚úÖ PDF Generator loaded successfully")
except ImportError as e:
    print(f"‚ö†Ô∏è  PDF Generator not available: {e}")
    print("Will save JSON and TXT only")
    PDF_AVAILABLE = False


class PipelineRunner:
    """
    Main pipeline orchestrator
    """
    
    def __init__(self):
        print("\n" + "="*60)
        print("Article Pipeline - Initializing")
        print("="*60 + "\n")
        
        # Initialize Google Sheets storage
        self.db = GoogleSheetsDB()
        
        # Initialize your agents
        print("ü§ñ Initializing Article Collector...")
        self.collector = CustomArticleCollector()
        
        print("ü§ñ Initializing Article Summarizer...")
        self.summarizer = ArticleSummarizer()
        
        print("\n‚úÖ Pipeline initialized successfully\n")
    
    def run_collection(self):
        """
        Step 1: Collect top 3 articles from each publication
        """
        print("\n" + "="*60)
        print("STEP 1: COLLECTING ARTICLES (Top 3 per Publication)")
        print("="*60 + "\n")
        
        try:
            # Run your collector
            articles = self.collector.collect_top_3_per_publication()
            
            if not articles:
                print("‚ö†Ô∏è  No articles collected")
                return [], []
            
            print(f"\n‚úÖ Collected {len(articles)} total articles")
            
            # Convert ArticleCandidate objects to dictionaries
            articles_data = []
            for idx, article in enumerate(articles):
                # Extract author properly
                author = article.author if hasattr(article, 'author') else 'Unknown'
                summary_text = article.summary if hasattr(article, 'summary') else ''
                
                article_dict = {
                    'id': f"article-{datetime.now().strftime('%Y%m%d')}-{idx+1}",
                    'title': article.title,
                    'url': article.url,
                    'publication': article.publication,
                    'journalist': author,
                    'author': author,  # For PDF generation
                    'summary': summary_text,
                }
                articles_data.append(article_dict)
            
            # Save to Google Sheets
            print("\nüíæ Saving to Google Sheets...")
            self.db.save_articles(articles_data)
            
            return articles, articles_data
            
        except Exception as e:
            print(f"‚ùå Error during collection: {str(e)}")
            import traceback
            traceback.print_exc()
            raise
    
    def generate_outputs(self, articles_data):
        """
        Generate TXT, JSON, and PDF outputs, then upload PDF to Google Drive
        """
        print("\n" + "="*60)
        print("STEP 2: GENERATING OUTPUT FILES")
        print("="*60)
        print(f"PDF_AVAILABLE: {PDF_AVAILABLE}")
        print(f"Number of articles: {len(articles_data)}")
        print(f"Current directory: {os.getcwd()}\n")
        
        if not articles_data:
            print("‚ö†Ô∏è  No data to generate outputs")
            return None, None, None
        
        # Create output directory
        os.makedirs('output', exist_ok=True)
        print(f"‚úÖ Output directory ready: {os.path.abspath('output')}\n")
        
        # Generate filenames
        date_str = datetime.now().strftime('%Y%m%d')
        txt_file = f"output/biweekly_roundup_{date_str}.txt"
        json_file = f"output/biweekly_roundup_{date_str}.json"
        pdf_file = f"output/biweekly_roundup_{date_str}.pdf"
        
        # Generate formatted text output
        print("üìù Generating TXT file...")
        formatted_text = self._generate_formatted_text(articles_data)
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write(formatted_text)
        print(f"‚úÖ Saved: {txt_file}")
        
        # Save JSON
        print("üìù Generating JSON file...")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(articles_data, f, indent=2, ensure_ascii=False)
        print(f"‚úÖ Saved: {json_file}")
        
        # Generate PDF
        pdf_path = None
        if PDF_AVAILABLE:
            try:
                print("üìù Generating PDF file...")
                pdf_gen = BiweeklyRoundupPDF()
                pdf_path = pdf_gen.generate_pdf(json_file, pdf_file)
                
                if pdf_path and os.path.exists(pdf_path):
                    print(f"‚úÖ PDF generated: {pdf_file}")
                    print(f"   File size: {os.path.getsize(pdf_path)} bytes")
                    
                    # NOW UPLOAD TO GOOGLE DRIVE
                    print("\n" + "="*60)
                    print("STEP 3: UPLOADING PDF TO GOOGLE DRIVE")
                    print("="*60 + "\n")
                    
                    try:
                        download_link, view_link = self.db.upload_pdf_to_drive(pdf_path)
                        
                        if download_link and view_link:
                            print(f"‚úÖ PDF uploaded to Google Drive successfully!")
                            print(f"   View link: {view_link}")
                            print(f"   Download link: {download_link}")
                            
                            # Save links to Metadata sheet
                            print("\nüíæ Saving PDF links to Metadata sheet...")
                            self.db.save_pdf_link(download_link, view_link)
                            print("‚úÖ PDF links saved to Google Sheets")
                        else:
                            print("‚ö†Ô∏è  Google Drive upload returned no links")
                            
                    except Exception as upload_error:
                        print(f"‚ùå Error uploading to Google Drive: {str(upload_error)}")
                        import traceback
                        traceback.print_exc()
                        print("\n‚ö†Ô∏è  Continuing without Drive upload - PDF still saved locally")
                else:
                    print(f"‚ö†Ô∏è  PDF generation did not create file at: {pdf_file}")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è  PDF generation failed: {e}")
                import traceback
                traceback.print_exc()
        else:
            print("‚ö†Ô∏è  PDF generation skipped (PDFGenerator not available)")
        
        return txt_file, json_file, pdf_path
    
    def _generate_formatted_text(self, articles_data):
        """Generate formatted text output"""
        output = []
        output.append("\nBI-WEEKLY READING ROUNDUP")
        output.append("=" * 60)
        output.append(f"Date: {datetime.now().strftime('%Y-%m-%d')}")
        output.append(f"Coverage Period: Last 14 days")
        output.append(f"Total Articles: {len(articles_data)}\n")
        
        # Group by publication
        by_publication = {}
        for article in articles_data:
            pub = article.get('publication', 'Unknown')
            if pub not in by_publication:
                by_publication[pub] = []
            by_publication[pub].append(article)
        
        output.append("By Publication:")
        for pub, items in sorted(by_publication.items(), key=lambda x: len(x[1]), reverse=True):
            output.append(f"  {pub}: {len(items)}")
        
        output.append("\n\nARTICLE SUMMARIES:")
        output.append("-" * 60)
        
        for article in articles_data:
            formatted = f"\n* [{article['title']}]({article['url']}) by {article['journalist']} - {article['summary']}"
            output.append(formatted)
        
        return "\n".join(output)
    
    def run_full_pipeline(self):
        """
        Run the complete pipeline
        """
        print("\n" + "="*60)
        print("üöÄ STARTING FULL PIPELINE")
        print("="*60)
        print(f"‚è∞ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        start_time = datetime.now()
        
        try:
            # Step 1: Collect articles
            articles, articles_data = self.run_collection()
            
            # Step 2: Generate outputs (TXT, JSON, PDF) and upload to Drive
            txt_file, json_file, pdf_file = self.generate_outputs(articles_data)
            
            # Summary
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            print("\n" + "="*60)
            print("‚úÖ PIPELINE COMPLETED SUCCESSFULLY")
            print("="*60)
            print(f"‚è±Ô∏è  Duration: {duration:.1f} seconds ({duration/60:.1f} minutes)")
            print(f"üìä Articles collected: {len(articles)}")
            print(f"üìÑ TXT file: {txt_file}")
            print(f"üìÑ JSON file: {json_file}")
            if pdf_file:
                print(f"üìÑ PDF file: {pdf_file}")
            print(f"‚è∞ End time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
            print("="*60 + "\n")
            
            return {
                'success': True,
                'articles_collected': len(articles),
                'duration_seconds': duration,
                'files': {
                    'txt': txt_file,
                    'json': json_file,
                    'pdf': pdf_file
                }
            }
            
        except Exception as e:
            print("\n" + "="*60)
            print("‚ùå PIPELINE FAILED")
            print("="*60)
            print(f"Error: {str(e)}")
            import traceback
            traceback.print_exc()
            print("="*60 + "\n")
            raise


def main():
    """
    Main entry point
    """
    try:
        runner = PipelineRunner()
        result = runner.run_full_pipeline()
        
        print("\nüéâ Pipeline completed successfully!")
        print(f"Check Google Sheets and Google Drive for results")
        
        # Exit with success
        sys.exit(0)
        
    except Exception as e:
        print(f"\n‚ùå Pipeline failed with error: {str(e)}")
        # Exit with error code
        sys.exit(1)


if __name__ == "__main__":
    main()